[INFO] [2022-04-24 14:35:25][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit01
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 14:35:26][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 14:35:26][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 14:35:26][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 14:35:26][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 14:35:26][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 14:35:27][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 14:35:27][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Discovered group coordinator prod-bigdata-pc14:6667 (id: 2147482643 rack: null)
[INFO] [2022-04-24 14:35:27][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Revoking previously assigned partitions []
[INFO] [2022-04-24 14:35:27][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] (Re-)joining group
[INFO] [2022-04-24 14:35:30][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Successfully joined group with generation 1
[INFO] [2022-04-24 14:35:30][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 14:35:30][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-1, groupId=transmit01] Resetting offset for partition test5-0 to offset 13950.
[WARN] [2022-04-24 14:36:19][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:21][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:23][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:26][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:28][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:31][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:34][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:37][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:40][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:43][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:46][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:50][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:53][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:56][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:59][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:02][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:05][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:08][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:11][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:14][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:17][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:20][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:23][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:26][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:29][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:32][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:35][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:38][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:41][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:45][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:48][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:51][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[INFO] [2022-04-24 15:08:11][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit01
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 15:08:12][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:08:12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:08:12][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:08:13][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:08:13][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:08:13][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:08:13][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Discovered group coordinator prod-bigdata-pc14:6667 (id: 2147482643 rack: null)
[INFO] [2022-04-24 15:08:13][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:08:13][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] (Re-)joining group
[INFO] [2022-04-24 15:08:16][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Successfully joined group with generation 1
[INFO] [2022-04-24 15:08:16][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 15:08:16][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-1, groupId=transmit01] Resetting offset for partition test5-0 to offset 13951.
[INFO] [2022-04-24 15:08:19][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 15:11:48][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit01
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 15:11:50][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:11:50][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:11:50][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:11:50][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:11:50][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:11:50][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:11:50][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Discovered group coordinator prod-bigdata-pc14:6667 (id: 2147482643 rack: null)
[INFO] [2022-04-24 15:11:50][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:11:50][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] (Re-)joining group
[INFO] [2022-04-24 15:11:53][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Successfully joined group with generation 3
[INFO] [2022-04-24 15:11:53][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 15:11:53][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-1, groupId=transmit01] Resetting offset for partition test5-0 to offset 13953.
[INFO] [2022-04-24 15:12:24][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 15:13:02][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 15:13:04][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:13:04][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:13:04][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:13:04][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:13:04][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:13:04][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 15:13:04][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:13:04][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 15:13:04][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:13:07][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 1
[INFO] [2022-04-24 15:13:07][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 15:13:07][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Resetting offset for partition test5-0 to offset 13954.
[INFO] [2022-04-24 15:13:11][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 15:16:00][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 15:16:02][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:16:02][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:16:02][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:16:02][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:16:02][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:16:02][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:16:02][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 15:16:02][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:16:02][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 15:16:05][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 3
[INFO] [2022-04-24 15:16:05][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 15:16:05][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Resetting offset for partition test5-0 to offset 13962.
[INFO] [2022-04-24 15:19:11][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 15:19:13][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:19:13][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:19:13][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:19:13][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:19:13][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:19:13][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:19:13][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 15:19:13][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:19:13][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 15:19:36][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 4
[INFO] [2022-04-24 15:19:36][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 15:38:00][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 15:38:02][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:38:02][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:38:02][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:38:02][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:38:02][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:38:02][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:38:02][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 15:38:02][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:38:02][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 15:38:05][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 1
[INFO] [2022-04-24 15:38:05][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 15:39:56][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 15:39:58][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:39:58][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:39:58][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:39:58][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:39:58][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:39:58][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:39:58][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 15:39:58][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:39:58][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 15:40:01][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 3
[INFO] [2022-04-24 15:40:01][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 15:40:01][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Resetting offset for partition test5-0 to offset 13963.
[INFO] [2022-04-24 15:40:10][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[WARN] [2022-04-24 15:40:37][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 15:40:39][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 15:40:42][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 15:40:44][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 15:40:47][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 15:40:50][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 15:40:53][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 15:40:56][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 15:40:59][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 15:41:02][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 15:41:05][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[INFO] [2022-04-24 15:45:57][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 15:45:58][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:45:58][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:45:58][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:45:58][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:45:58][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:45:59][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:45:59][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 15:45:59][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:45:59][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 15:46:02][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 5
[INFO] [2022-04-24 15:46:02][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 15:46:02][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Resetting offset for partition test5-0 to offset 13967.
[INFO] [2022-04-24 15:50:10][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 15:50:12][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:50:12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:50:12][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:50:12][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:50:12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:50:12][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:50:12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 15:50:12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:50:12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 15:50:29][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 6
[INFO] [2022-04-24 15:50:29][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 15:50:29][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Resetting offset for partition test5-0 to offset 13970.
[INFO] [2022-04-24 15:51:12][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 15:54:09][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 15:54:11][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:54:11][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:54:11][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:54:11][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:54:11][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:54:11][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:54:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 15:54:11][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:54:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 15:54:14][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 8
[INFO] [2022-04-24 15:54:14][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 15:54:21][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 15:56:10][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[WARN] [2022-04-24 15:56:12][org.apache.kafka.clients.consumer.ConsumerConfig]The configuration 'ack-mode' was supplied but isn't a known config.
[INFO] [2022-04-24 15:56:12][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:56:12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:56:12][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:56:12][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:56:12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:56:12][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:56:12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 15:56:12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:56:12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 15:56:32][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 9
[INFO] [2022-04-24 15:56:32][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 15:56:33][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 15:57:02][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 15:57:04][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:57:04][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:57:04][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:57:04][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:57:04][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:57:04][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:57:04][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 15:57:04][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:57:04][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 15:57:31][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[WARN] [2022-04-24 15:57:33][org.apache.kafka.clients.consumer.ConsumerConfig]The configuration 'ack-mode' was supplied but isn't a known config.
[INFO] [2022-04-24 15:57:33][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:57:33][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:57:33][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:57:33][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:57:33][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:57:33][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:57:33][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 15:57:33][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:57:33][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 15:57:51][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 11
[INFO] [2022-04-24 15:57:51][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 15:57:51][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 15:59:08][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 15:59:10][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:59:10][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:59:10][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:59:10][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:59:10][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:59:10][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:59:10][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 15:59:10][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:59:10][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 15:59:24][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 12
[INFO] [2022-04-24 15:59:24][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 15:59:24][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 15:59:46][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 15:59:48][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:59:48][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:59:48][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:59:48][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:59:48][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:59:48][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:59:48][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 15:59:48][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:59:48][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:00:06][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 13
[INFO] [2022-04-24 16:00:06][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:00:06][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 16:07:39][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 16:07:41][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:07:41][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:07:41][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 16:07:41][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:07:41][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:07:41][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 16:07:41][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 16:07:41][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 16:07:41][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:07:44][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 15
[INFO] [2022-04-24 16:07:44][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:07:44][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 16:08:09][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 16:08:11][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:08:11][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:08:11][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 16:08:11][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:08:11][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:08:11][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 16:08:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 16:08:11][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 16:08:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:08:32][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 16
[INFO] [2022-04-24 16:08:32][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:08:32][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 16:11:28][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 16:11:30][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:11:30][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:11:30][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 16:11:30][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:11:30][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:11:30][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 16:11:30][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 16:11:30][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 16:11:30][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:11:33][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 18
[INFO] [2022-04-24 16:11:33][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:11:33][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 16:17:34][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 16:17:36][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:17:36][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:17:36][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 16:17:36][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:17:36][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:17:37][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 16:17:37][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 16:17:37][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 16:17:37][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:17:40][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 20
[INFO] [2022-04-24 16:17:40][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:17:40][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[ERROR] [2022-04-24 16:17:41][org.apache.kafka.clients.producer.internals.ProducerBatch]Error executing user-provided callback on message for topic-partition 'test3-0'
java.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:2215)
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquireAndEnsureOpen(KafkaConsumer.java:2199)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1453)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1431)
	at kl.transmit.TransmitJob$1.onCompletion(TransmitJob.java:62)
	at org.apache.kafka.clients.producer.KafkaProducer$InterceptorCallback.onCompletion(KafkaProducer.java:1235)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.completeFutureAndFireCallbacks(ProducerBatch.java:201)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.done(ProducerBatch.java:187)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:599)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:575)
	at org.apache.kafka.clients.producer.internals.Sender.handleProduceResponse(Sender.java:485)
	at org.apache.kafka.clients.producer.internals.Sender.access$100(Sender.java:74)
	at org.apache.kafka.clients.producer.internals.Sender$1.onComplete(Sender.java:700)
	at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)
	at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:532)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:524)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:239)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:163)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] [2022-04-24 16:17:41][org.apache.kafka.clients.producer.internals.ProducerBatch]Error executing user-provided callback on message for topic-partition 'test3-0'
java.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:2215)
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquireAndEnsureOpen(KafkaConsumer.java:2199)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1453)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1431)
	at kl.transmit.TransmitJob$1.onCompletion(TransmitJob.java:62)
	at org.apache.kafka.clients.producer.KafkaProducer$InterceptorCallback.onCompletion(KafkaProducer.java:1235)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.completeFutureAndFireCallbacks(ProducerBatch.java:201)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.done(ProducerBatch.java:187)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:599)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:575)
	at org.apache.kafka.clients.producer.internals.Sender.handleProduceResponse(Sender.java:485)
	at org.apache.kafka.clients.producer.internals.Sender.access$100(Sender.java:74)
	at org.apache.kafka.clients.producer.internals.Sender$1.onComplete(Sender.java:700)
	at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)
	at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:532)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:524)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:239)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:163)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] [2022-04-24 16:17:41][org.apache.kafka.clients.producer.internals.ProducerBatch]Error executing user-provided callback on message for topic-partition 'test3-0'
java.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:2215)
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquireAndEnsureOpen(KafkaConsumer.java:2199)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1453)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1431)
	at kl.transmit.TransmitJob$1.onCompletion(TransmitJob.java:62)
	at org.apache.kafka.clients.producer.KafkaProducer$InterceptorCallback.onCompletion(KafkaProducer.java:1235)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.completeFutureAndFireCallbacks(ProducerBatch.java:201)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.done(ProducerBatch.java:187)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:599)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:575)
	at org.apache.kafka.clients.producer.internals.Sender.handleProduceResponse(Sender.java:485)
	at org.apache.kafka.clients.producer.internals.Sender.access$100(Sender.java:74)
	at org.apache.kafka.clients.producer.internals.Sender$1.onComplete(Sender.java:700)
	at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)
	at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:532)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:524)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:239)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:163)
	at java.lang.Thread.run(Thread.java:748)
[INFO] [2022-04-24 16:26:19][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 16:26:20][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:26:20][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:26:20][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 16:26:20][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:26:20][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:26:21][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 16:26:21][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 16:26:21][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 16:26:21][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:26:44][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 21
[INFO] [2022-04-24 16:26:44][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:26:44][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[ERROR] [2022-04-24 16:26:45][org.apache.kafka.clients.producer.internals.ProducerBatch]Error executing user-provided callback on message for topic-partition 'test3-0'
java.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:2215)
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquireAndEnsureOpen(KafkaConsumer.java:2199)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1453)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1431)
	at kl.transmit.TransmitJob$1.onCompletion(TransmitJob.java:65)
	at org.apache.kafka.clients.producer.KafkaProducer$InterceptorCallback.onCompletion(KafkaProducer.java:1235)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.completeFutureAndFireCallbacks(ProducerBatch.java:201)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.done(ProducerBatch.java:187)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:599)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:575)
	at org.apache.kafka.clients.producer.internals.Sender.handleProduceResponse(Sender.java:485)
	at org.apache.kafka.clients.producer.internals.Sender.access$100(Sender.java:74)
	at org.apache.kafka.clients.producer.internals.Sender$1.onComplete(Sender.java:700)
	at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)
	at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:532)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:524)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:239)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:163)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] [2022-04-24 16:26:45][org.apache.kafka.clients.producer.internals.ProducerBatch]Error executing user-provided callback on message for topic-partition 'test3-0'
java.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:2215)
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquireAndEnsureOpen(KafkaConsumer.java:2199)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1453)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1431)
	at kl.transmit.TransmitJob$1.onCompletion(TransmitJob.java:65)
	at org.apache.kafka.clients.producer.KafkaProducer$InterceptorCallback.onCompletion(KafkaProducer.java:1235)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.completeFutureAndFireCallbacks(ProducerBatch.java:201)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.done(ProducerBatch.java:187)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:599)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:575)
	at org.apache.kafka.clients.producer.internals.Sender.handleProduceResponse(Sender.java:485)
	at org.apache.kafka.clients.producer.internals.Sender.access$100(Sender.java:74)
	at org.apache.kafka.clients.producer.internals.Sender$1.onComplete(Sender.java:700)
	at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)
	at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:532)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:524)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:239)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:163)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] [2022-04-24 16:26:45][org.apache.kafka.clients.producer.internals.ProducerBatch]Error executing user-provided callback on message for topic-partition 'test3-0'
java.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:2215)
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquireAndEnsureOpen(KafkaConsumer.java:2199)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1453)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1431)
	at kl.transmit.TransmitJob$1.onCompletion(TransmitJob.java:65)
	at org.apache.kafka.clients.producer.KafkaProducer$InterceptorCallback.onCompletion(KafkaProducer.java:1235)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.completeFutureAndFireCallbacks(ProducerBatch.java:201)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.done(ProducerBatch.java:187)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:599)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:575)
	at org.apache.kafka.clients.producer.internals.Sender.handleProduceResponse(Sender.java:485)
	at org.apache.kafka.clients.producer.internals.Sender.access$100(Sender.java:74)
	at org.apache.kafka.clients.producer.internals.Sender$1.onComplete(Sender.java:700)
	at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)
	at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:532)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:524)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:239)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:163)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] [2022-04-24 16:26:45][org.apache.kafka.clients.producer.internals.ProducerBatch]Error executing user-provided callback on message for topic-partition 'test3-0'
java.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:2215)
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquireAndEnsureOpen(KafkaConsumer.java:2199)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1453)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1431)
	at kl.transmit.TransmitJob$1.onCompletion(TransmitJob.java:65)
	at org.apache.kafka.clients.producer.KafkaProducer$InterceptorCallback.onCompletion(KafkaProducer.java:1235)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.completeFutureAndFireCallbacks(ProducerBatch.java:201)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.done(ProducerBatch.java:187)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:599)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:575)
	at org.apache.kafka.clients.producer.internals.Sender.handleProduceResponse(Sender.java:485)
	at org.apache.kafka.clients.producer.internals.Sender.access$100(Sender.java:74)
	at org.apache.kafka.clients.producer.internals.Sender$1.onComplete(Sender.java:700)
	at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)
	at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:532)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:524)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:239)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:163)
	at java.lang.Thread.run(Thread.java:748)
[INFO] [2022-04-24 16:29:25][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 16:29:27][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:29:27][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:29:27][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 16:29:27][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:29:27][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:29:27][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 16:29:27][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 16:29:27][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 16:29:27][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:29:30][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 23
[INFO] [2022-04-24 16:29:30][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:29:31][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 16:36:57][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 16:36:58][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:36:58][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:36:58][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 16:36:59][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:36:59][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:36:59][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 16:36:59][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 16:36:59][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 16:36:59][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:37:02][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 25
[INFO] [2022-04-24 16:37:02][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:37:18][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 16:38:09][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 16:38:11][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:38:11][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:38:11][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 16:38:11][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:38:11][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:38:11][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 16:38:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 16:38:11][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 16:38:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:38:14][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 27
[INFO] [2022-04-24 16:38:14][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:38:14][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 16:39:11][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 16:39:13][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:39:13][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:39:13][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 16:39:13][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:39:13][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:39:13][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 16:39:13][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 16:39:13][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 16:39:13][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:39:20][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 28
[INFO] [2022-04-24 16:39:20][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:39:20][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 16:41:02][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 16:41:03][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:41:03][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:41:03][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 16:41:03][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:41:03][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:41:04][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 16:41:04][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 16:41:04][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 16:41:04][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:41:23][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 29
[INFO] [2022-04-24 16:41:23][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:41:45][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 16:41:46][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:41:46][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:41:47][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 16:41:47][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:41:47][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:41:47][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 16:41:47][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 16:41:47][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 16:41:47][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:42:05][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 30
[INFO] [2022-04-24 16:42:05][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:42:06][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 16:48:02][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 16:48:04][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:48:04][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:48:04][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 16:48:04][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:48:04][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:48:04][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 16:48:04][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 16:48:04][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 16:48:04][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:48:12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 31
[INFO] [2022-04-24 16:48:12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:48:12][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 16:49:15][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 16:49:17][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:49:17][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:49:17][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 16:49:17][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:49:17][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:49:17][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 16:49:17][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 16:49:17][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 16:49:17][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:49:31][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 32
[INFO] [2022-04-24 16:49:31][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:49:31][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 17:13:51][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 17:13:52][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 17:13:52][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 17:13:52][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 17:13:53][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 17:13:53][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 17:13:53][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 17:13:53][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 17:13:53][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 17:13:53][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 17:14:16][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 33
[INFO] [2022-04-24 17:14:16][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 17:14:16][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[WARN] [2022-04-24 17:14:55][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:14:57][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:00][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:02][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:05][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:08][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:11][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:15][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:18][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:21][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:24][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:27][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:30][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:33][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:36][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:39][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:42][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:45][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:47][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Error while fetching metadata with correlation id 11 : {test3=LEADER_NOT_AVAILABLE}
[WARN] [2022-04-24 17:15:47][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Got error produce response with correlation id 12 on topic-partition test3-0, retrying (9 attempts left). Error: NOT_LEADER_FOR_PARTITION
[WARN] [2022-04-24 17:15:47][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Received invalid metadata error in produce request on partition test3-0 due to org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.. Going to request metadata update now
[WARN] [2022-04-24 17:15:47][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Got error produce response with correlation id 14 on topic-partition test3-0, retrying (8 attempts left). Error: NOT_LEADER_FOR_PARTITION
[WARN] [2022-04-24 17:15:47][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Received invalid metadata error in produce request on partition test3-0 due to org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.. Going to request metadata update now
[WARN] [2022-04-24 17:16:06][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:09][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:11][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:13][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:16][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:19][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:22][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:26][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:29][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:31][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:34][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:37][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:41][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:44][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:47][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:50][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:52][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Got error produce response with correlation id 21 on topic-partition test3-0, retrying (9 attempts left). Error: NOT_LEADER_FOR_PARTITION
[WARN] [2022-04-24 17:16:52][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Received invalid metadata error in produce request on partition test3-0 due to org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.. Going to request metadata update now
[WARN] [2022-04-24 17:16:52][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Got error produce response with correlation id 23 on topic-partition test3-0, retrying (8 attempts left). Error: NOT_LEADER_FOR_PARTITION
[WARN] [2022-04-24 17:16:52][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Received invalid metadata error in produce request on partition test3-0 due to org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.. Going to request metadata update now
[WARN] [2022-04-24 17:16:52][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Got error produce response with correlation id 25 on topic-partition test3-0, retrying (7 attempts left). Error: NOT_LEADER_FOR_PARTITION
[WARN] [2022-04-24 17:16:52][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Received invalid metadata error in produce request on partition test3-0 due to org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.. Going to request metadata update now
[INFO] [2022-04-24 17:20:27][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 17:20:28][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 17:20:28][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 17:20:28][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 17:20:28][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 17:20:28][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 17:20:29][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 17:20:29][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 17:20:29][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 17:20:29][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 17:20:32][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 35
[INFO] [2022-04-24 17:20:32][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 17:20:32][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 17:52:40][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-24 17:52:40][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 17:52:42][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 17:52:42][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 17:52:42][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 17:52:42][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 17:52:42][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 17:52:42][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-24 17:52:42][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-24 17:52:42][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 17:52:42][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 17:52:42][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 17:52:42][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 17:52:46][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 37
[INFO] [2022-04-24 17:52:46][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 17:53:14][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-25 15:51:45][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit01
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-25 15:51:45][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit01
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-25 15:51:45][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit01
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-25 15:51:46][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-25 15:51:46][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-25 15:51:46][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-25 15:51:46][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-25 15:51:46][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-25 15:51:46][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-25 15:51:47][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-25 15:51:47][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-25 15:51:47][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-2, groupId=transmit01] Discovered group coordinator Mi-Lixz:9092 (id: 2147483647 rack: null)
[INFO] [2022-04-25 15:51:47][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Discovered group coordinator Mi-Lixz:9092 (id: 2147483647 rack: null)
[INFO] [2022-04-25 15:51:47][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Revoking previously assigned partitions []
[INFO] [2022-04-25 15:51:47][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-2, groupId=transmit01] Revoking previously assigned partitions []
[INFO] [2022-04-25 15:51:47][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] (Re-)joining group
[INFO] [2022-04-25 15:51:47][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-2, groupId=transmit01] (Re-)joining group
[INFO] [2022-04-25 15:51:47][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-2, groupId=transmit01] Successfully joined group with generation 1
[INFO] [2022-04-25 15:51:47][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-2, groupId=transmit01] Setting newly assigned partitions []
[INFO] [2022-04-25 15:51:47][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Successfully joined group with generation 1
[INFO] [2022-04-25 15:51:47][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Setting newly assigned partitions [test3-0]
[INFO] [2022-04-25 15:51:47][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-1, groupId=transmit01] Resetting offset for partition test3-0 to offset 105.
[INFO] [2022-04-25 15:52:18][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit01
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-25 15:52:18][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit01
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-25 15:52:18][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit01
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-25 15:52:20][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-25 15:52:20][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-25 15:52:20][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-25 15:52:20][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-25 15:52:20][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-25 15:52:20][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-25 15:52:20][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-25 15:52:20][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-25 15:52:20][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-25 15:52:20][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-3, groupId=transmit01] Discovered group coordinator Mi-Lixz:9092 (id: 2147483647 rack: null)
[INFO] [2022-04-25 15:52:20][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-2, groupId=transmit01] Discovered group coordinator Mi-Lixz:9092 (id: 2147483647 rack: null)
[INFO] [2022-04-25 15:52:20][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Discovered group coordinator Mi-Lixz:9092 (id: 2147483647 rack: null)
[INFO] [2022-04-25 15:52:20][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-3, groupId=transmit01] Revoking previously assigned partitions []
[INFO] [2022-04-25 15:52:20][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-2, groupId=transmit01] Revoking previously assigned partitions []
[INFO] [2022-04-25 15:52:20][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Revoking previously assigned partitions []
[INFO] [2022-04-25 15:52:20][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-2, groupId=transmit01] (Re-)joining group
[INFO] [2022-04-25 15:52:20][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-3, groupId=transmit01] (Re-)joining group
[INFO] [2022-04-25 15:52:20][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] (Re-)joining group
[INFO] [2022-04-25 15:52:29][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-2, groupId=transmit01] Successfully joined group with generation 2
[INFO] [2022-04-25 15:52:29][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-2, groupId=transmit01] Setting newly assigned partitions []
[INFO] [2022-04-25 15:52:29][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-3, groupId=transmit01] Successfully joined group with generation 2
[INFO] [2022-04-25 15:52:29][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-3, groupId=transmit01] Setting newly assigned partitions []
[INFO] [2022-04-25 15:52:29][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Successfully joined group with generation 2
[INFO] [2022-04-25 15:52:29][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Setting newly assigned partitions [test3-0]
[INFO] [2022-04-25 15:52:29][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-1, groupId=transmit01] Resetting offset for partition test3-0 to offset 105.
[INFO] [2022-04-25 15:59:09][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit0Thread-0
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-25 15:59:09][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit0Thread-1
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-25 15:59:09][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit0Thread-2
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-25 15:59:10][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-25 15:59:10][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-25 15:59:10][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-25 15:59:10][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-25 15:59:10][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-25 15:59:10][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-2, groupId=transmit0Thread-1] Discovered group coordinator Mi-Lixz:9092 (id: 2147483647 rack: null)
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit0Thread-0] Discovered group coordinator Mi-Lixz:9092 (id: 2147483647 rack: null)
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-3, groupId=transmit0Thread-2] Discovered group coordinator Mi-Lixz:9092 (id: 2147483647 rack: null)
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-3, groupId=transmit0Thread-2] Revoking previously assigned partitions []
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit0Thread-0] Revoking previously assigned partitions []
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-2, groupId=transmit0Thread-1] Revoking previously assigned partitions []
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit0Thread-0] (Re-)joining group
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-3, groupId=transmit0Thread-2] (Re-)joining group
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-2, groupId=transmit0Thread-1] (Re-)joining group
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit0Thread-0] Successfully joined group with generation 1
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit0Thread-0] Setting newly assigned partitions [test3-0]
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-2, groupId=transmit0Thread-1] Successfully joined group with generation 1
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-2, groupId=transmit0Thread-1] Setting newly assigned partitions [test3-0]
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-1, groupId=transmit0Thread-0] Resetting offset for partition test3-0 to offset 106.
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-2, groupId=transmit0Thread-1] Resetting offset for partition test3-0 to offset 106.
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-3, groupId=transmit0Thread-2] Successfully joined group with generation 1
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-3, groupId=transmit0Thread-2] Setting newly assigned partitions [test3-0]
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-3, groupId=transmit0Thread-2] Resetting offset for partition test3-0 to offset 106.
[INFO] [2022-04-25 16:45:26][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit0main
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-25 16:45:28][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-25 16:45:28][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-25 16:45:28][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-25 16:49:10][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit0main
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-25 16:49:12][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-25 16:49:12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-25 16:49:12][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-25 16:53:55][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group01
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-25 16:53:57][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-25 16:53:57][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-25 16:53:57][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-25 16:54:23][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group01
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-25 16:54:23][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-25 16:54:23][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-25 16:54:24][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-25 16:55:45][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = group01
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-25 16:55:47][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-25 16:55:47][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-25 16:55:47][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 11:30:16][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 11:30:16][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 11:30:18][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 11:30:18][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 11:30:18][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 11:30:18][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 11:30:18][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 11:30:18][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-26 11:30:18][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-26 11:30:18][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 11:30:18][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-26 11:30:18][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-26 11:30:18][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-26 11:30:21][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 39
[INFO] [2022-04-26 11:30:21][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-26 11:30:27][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 13:14:40][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 13:14:41][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:14:42][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:14:42][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:14:43][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:14:43][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:14:43][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:14:43][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:14:43][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 13:14:43][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-25 10:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[ERROR] [2022-04-26 13:14:43][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 13:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 13:16:42][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 13:16:42][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:16:42][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:16:42][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:16:43][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:16:43][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:16:43][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:16:43][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:16:43][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 13:19:53][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-25 10:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[ERROR] [2022-04-26 13:19:53][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 13:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 13:23:53][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 13:23:53][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:23:53][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:23:53][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:23:53][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:23:53][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:23:54][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:23:54][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:23:54][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 13:24:31][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-25 10:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[ERROR] [2022-04-26 13:24:31][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 13:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 13:24:31][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:24:31][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:24:31][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:25:39][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 13:25:39][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:25:39][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:25:39][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:25:40][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:25:40][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:25:40][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:25:40][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:25:40][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 13:26:03][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-25 10:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[ERROR] [2022-04-26 13:26:03][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 13:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 13:26:03][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:26:03][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:26:03][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:27:11][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 13:27:11][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:27:11][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:27:11][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:27:11][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:27:11][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:27:11][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:27:11][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:27:11][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:27:15][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:27:15][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:27:15][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:27:17][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:27:35][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 13:27:35][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:27:35][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:27:35][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 13:27:35][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 13:28:54][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 13:28:54][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:28:54][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:28:54][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:28:55][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:28:55][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:28:55][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:28:55][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:28:55][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:28:55][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:28:55][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:28:55][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:28:55][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:29:35][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 13:29:35][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:29:35][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:29:35][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 13:29:35][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[WARN] [2022-04-26 13:30:18][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Got error produce response with correlation id 3 on topic-partition test3-0, retrying (9 attempts left). Error: NETWORK_EXCEPTION
[WARN] [2022-04-26 13:30:21][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Received invalid metadata error in produce request on partition test3-0 due to org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.. Going to request metadata update now
[INFO] [2022-04-26 13:30:29][org.apache.kafka.clients.FetchSessionHandler][Consumer clientId=consumer-3, groupId=transmit-hdp-dev] Error sending fetch request (sessionId=140574796, epoch=1) to node 1006: org.apache.kafka.common.errors.DisconnectException.
[INFO] [2022-04-26 13:31:33][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 13:31:33][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:31:35][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:31:35][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:31:36][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:31:36][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:31:36][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:31:36][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:31:36][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:31:36][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:31:36][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:31:36][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:31:36][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:31:36][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 13:31:36][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:31:36][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:31:36][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 13:31:36][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 13:31:36][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：1条
[INFO] [2022-04-26 13:33:28][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 13:33:28][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:33:30][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:33:30][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:33:30][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:33:30][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:33:30][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:33:30][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:33:30][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:33:30][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:33:30][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:33:30][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:33:30][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:33:30][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 13:33:30][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:33:30][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:33:30][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 13:33:30][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 13:33:30][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：1条
[INFO] [2022-04-26 13:35:25][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 13:35:25][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:35:27][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:35:27][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:35:27][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:35:27][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:35:27][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:35:27][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:35:27][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:35:27][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:35:27][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:35:27][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:35:27][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:35:27][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 13:35:27][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:35:27][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:35:27][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 13:35:27][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：0条
[INFO] [2022-04-26 13:36:07][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 13:36:07][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:36:09][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:36:09][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:36:09][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:36:09][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:36:09][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:36:09][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:36:09][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:36:09][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:36:09][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:36:09][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:36:09][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:37:31][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 13:37:37][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:37:37][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:37:37][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:37:46][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:38:34][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:38:34][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:38:34][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:38:35][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:38:41][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:38:41][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:38:41][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:38:43][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:39:03][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 13:39:08][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:39:08][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:39:08][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:39:08][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:39:12][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:39:12][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:39:12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:39:12][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:39:12][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:39:12][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:39:12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:39:19][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:40:15][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 13:40:15][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:40:15][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:40:15][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:40:16][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:40:16][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:40:16][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:40:16][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:40:16][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:40:16][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:40:16][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:40:16][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:40:34][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:44:56][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 13:44:56][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:44:56][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:44:56][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:44:57][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:44:57][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:44:57][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:44:57][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:44:57][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:44:57][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:44:57][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:44:57][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:45:02][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 13:45:24][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 14:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 13:52:13][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 13:52:13][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:52:13][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:52:13][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:52:14][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:52:14][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:52:14][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:52:14][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:52:14][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:52:14][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:52:14][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:52:14][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:52:21][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 13:52:34][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 14:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 13:53:20][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 13:53:20][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:53:20][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:53:20][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 13:53:20][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：0条
[INFO] [2022-04-26 13:54:35][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 13:54:36][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:54:37][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:54:37][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:54:37][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:54:38][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:54:38][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:54:38][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:54:38][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:54:38][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:54:38][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:54:38][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:54:38][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 13:54:38][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 14:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 13:54:38][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 13:54:38][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:54:38][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:54:38][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 13:54:38][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：0条
[INFO] [2022-04-26 13:55:56][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 13:55:56][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:55:58][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:55:58][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:55:58][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:55:58][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:55:58][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:55:58][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:55:58][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:55:58][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:55:58][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:55:58][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:55:58][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 13:55:58][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 14:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 13:55:59][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 13:55:59][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:55:59][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:55:59][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 13:55:59][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：0条
[INFO] [2022-04-26 13:56:46][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 13:56:46][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:56:48][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:56:48][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:56:48][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:56:48][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:56:48][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:56:48][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:56:48][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:56:48][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:56:48][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:56:48][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:56:48][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 13:56:48][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 14:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 13:56:48][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 13:56:48][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:56:48][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:56:48][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 13:56:48][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：0条
[INFO] [2022-04-26 13:57:24][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 13:57:24][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:57:25][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:57:25][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:57:25][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:57:25][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:57:25][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:57:25][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:57:25][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:57:25][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:57:25][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:57:25][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:57:29][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 13:57:29][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 14:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 13:57:29][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 13:57:29][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:57:29][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:57:36][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 13:57:57][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：0条
[INFO] [2022-04-26 13:59:14][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 13:59:14][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:59:15][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:59:15][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:59:15][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:59:15][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:59:15][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:59:15][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:59:15][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 13:59:15][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 13:59:15][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:59:15][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:59:15][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 13:59:15][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 14:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 13:59:54][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 13:59:54][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 13:59:54][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 13:59:54][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 13:59:54][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：0条
[INFO] [2022-04-26 14:01:19][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:01:19][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:01:21][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:01:21][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:01:21][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:01:21][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:01:21][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:01:21][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:01:21][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:01:21][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:01:21][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:01:21][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:01:21][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 14:01:21][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 14:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 14:01:21][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:01:21][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:01:21][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:01:21][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:01:21][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:02:32][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:02:32][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:02:34][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:02:34][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:02:34][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:02:34][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:02:34][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:02:34][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:02:34][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:02:34][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:02:34][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:02:34][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:02:34][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 14:02:34][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 14:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 14:02:34][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:02:34][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:02:34][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:02:34][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:02:34][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:03:32][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:03:32][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:03:32][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:03:32][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:03:33][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:03:33][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:03:33][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:03:33][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:03:33][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:03:33][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:03:33][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:03:33][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:03:33][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 14:03:33][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 14:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 14:03:33][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:03:33][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:03:33][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:03:33][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:03:51][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:06:08][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:06:08][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:06:10][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:06:10][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:06:10][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:06:10][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:06:10][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:06:10][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:06:10][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:06:10][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:06:10][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:06:10][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:06:10][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 14:06:10][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 14:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 14:06:10][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:06:10][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:06:10][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:06:10][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:06:10][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:07:01][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:07:01][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:07:03][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:07:03][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:07:03][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:07:03][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:07:03][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:07:03][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:07:03][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:07:03][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:07:03][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:07:03][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:07:03][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 14:07:03][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 14:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 14:07:03][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:07:03][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:07:03][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:07:03][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:07:03][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:07:03][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：24785条
[INFO] [2022-04-26 14:07:34][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:07:34][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:07:36][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:07:36][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:07:36][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:07:36][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:07:36][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:07:36][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:07:36][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:07:36][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:07:36][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:07:36][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:07:36][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 14:07:36][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 14:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 14:07:36][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:07:36][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:07:36][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:07:36][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:07:36][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:07:36][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：4条
[INFO] [2022-04-26 14:21:01][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:21:01][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:21:03][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:21:03][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:21:03][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:21:03][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:21:03][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:21:03][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:21:03][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:21:03][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:21:03][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:21:03][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:21:03][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 14:21:03][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 14:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 14:21:03][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:21:03][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:21:03][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:21:03][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:21:03][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:21:03][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：4条
[INFO] [2022-04-26 14:21:35][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:21:35][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:21:36][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:21:36][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:21:37][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:21:37][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:21:37][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:21:37][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:21:37][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:21:37][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:21:37][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:21:37][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:21:37][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 14:21:37][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 14:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 14:21:37][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:21:37][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:21:37][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:21:37][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:21:37][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:21:37][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：4条
[INFO] [2022-04-26 14:21:57][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:21:57][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:21:59][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:21:59][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:21:59][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:21:59][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:21:59][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:21:59][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:21:59][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:21:59][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:21:59][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:21:59][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:21:59][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 14:21:59][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 14:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 14:21:59][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:21:59][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:21:59][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:21:59][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:21:59][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:21:59][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：4条
[INFO] [2022-04-26 14:23:24][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:23:24][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:23:26][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:23:26][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:23:26][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:23:26][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:23:26][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:23:26][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:23:26][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:23:26][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:23:26][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:23:26][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:23:26][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 14:23:26][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 14:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 14:23:26][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:23:26][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:23:26][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:23:26][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:23:26][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:23:26][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：4条
[INFO] [2022-04-26 14:24:11][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:24:11][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:24:13][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:24:13][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:24:13][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:24:13][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:24:13][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:24:13][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:24:13][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:24:13][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:24:13][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:24:13][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:24:13][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 14:24:13][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 14:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 14:24:13][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:24:13][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:24:13][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:24:13][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:24:13][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:24:13][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：4条
[INFO] [2022-04-26 14:24:55][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:24:55][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:24:57][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:24:57][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:24:57][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:24:57][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:24:57][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:24:57][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:24:57][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:24:57][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:24:57][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:24:57][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:24:57][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 14:24:57][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 14:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 14:24:57][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:24:57][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:24:57][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:24:57][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:24:57][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:24:57][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：4条
[INFO] [2022-04-26 14:25:38][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:25:38][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:25:40][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:25:40][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:25:40][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:25:40][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:25:40][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:25:40][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:25:40][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:25:40][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:25:40][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:25:40][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:25:40][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 14:25:40][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 14:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 14:25:40][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:25:40][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:25:40][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:25:40][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:25:40][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:25:44][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：4条
[INFO] [2022-04-26 14:26:56][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:26:56][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:26:58][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:26:58][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:26:58][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:26:58][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:26:58][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:26:58][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:26:58][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:26:58][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:26:58][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:26:58][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:26:58][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 14:26:58][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 14:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 14:26:58][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:26:58][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:26:58][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:26:58][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:26:58][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:26:58][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：4条
[INFO] [2022-04-26 14:27:21][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:27:21][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:27:23][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:27:23][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:27:23][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:27:23][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:27:23][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:27:23][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:27:23][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:27:23][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:27:23][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:27:23][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:27:23][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 14:27:23][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 14:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 14:27:23][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:27:23][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:27:23][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:27:23][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:27:23][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:27:23][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：4条
[INFO] [2022-04-26 14:28:39][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:28:40][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:28:41][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:28:41][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:28:42][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:28:42][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:28:42][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:28:42][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:28:42][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:28:42][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:28:42][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:28:42][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:28:42][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 14:28:42][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 14:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 14:28:42][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:28:42][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:28:42][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:28:42][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:28:42][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:28:42][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：4条
[INFO] [2022-04-26 14:29:49][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:29:49][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:29:51][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:29:51][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:29:51][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:29:51][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:29:51][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:29:51][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:29:51][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:29:51][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:29:51][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:29:51][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:29:51][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 14:29:51][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 14:00:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 14:29:51][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:29:51][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:29:51][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:29:51][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:29:51][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:29:51][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：4条
[INFO] [2022-04-26 14:30:28][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:30:28][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:30:30][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:30:30][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:30:30][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:30:30][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:30:30][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:30:30][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:30:30][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:30:30][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:30:30][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:30:30][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:30:30][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 14:30:30][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 14:40:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 14:30:30][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:30:30][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:30:30][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:30:30][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:30:30][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:30:31][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：14条
[INFO] [2022-04-26 14:30:57][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:30:57][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:30:59][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:30:59][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:30:59][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:30:59][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:30:59][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:30:59][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:30:59][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:30:59][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:30:59][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:30:59][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:30:59][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 14:30:59][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 14:40:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 14:30:59][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:31:00][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:31:00][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:31:00][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:31:00][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:31:00][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：10条
[INFO] [2022-04-26 14:31:32][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:31:32][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:31:34][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:31:34][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:31:34][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:31:34][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:31:34][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:31:34][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:31:34][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:31:34][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:31:34][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:31:34][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:31:34][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[ERROR] [2022-04-26 14:31:34][kl.utils.KafkaUtil]无法获取时间戳为[2022-04-26 14:40:00]的偏移量，请确认当前主题在该时间戳上是否有数据
[INFO] [2022-04-26 14:31:34][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:31:34][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:31:34][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:31:34][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:31:34][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:31:35][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：10条
[INFO] [2022-04-26 14:31:55][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:31:55][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:31:57][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:31:57][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:31:57][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:31:57][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:31:57][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:31:57][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:31:57][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:31:57][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:31:57][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:31:57][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:31:57][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:31:57][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:31:57][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:31:57][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:31:57][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:31:57][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:31:58][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：10条
[INFO] [2022-04-26 14:32:11][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:32:11][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:32:13][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:32:13][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:32:13][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:32:13][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:32:13][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:32:13][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:32:13][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:32:13][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:32:13][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:32:13][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:32:13][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:32:13][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:32:13][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:32:13][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:32:13][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:32:13][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:32:14][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：10条
[INFO] [2022-04-26 14:32:41][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:32:41][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:32:43][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:32:43][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:32:43][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:32:43][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:32:43][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:32:43][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:32:43][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:32:44][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:32:44][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:32:44][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:32:44][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:32:44][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:32:44][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:32:44][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:32:44][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:32:44][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:32:44][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：15条
[INFO] [2022-04-26 14:34:08][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:34:08][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:34:08][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:34:08][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:34:09][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:34:13][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:34:13][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:34:13][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:34:13][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:34:25][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:34:25][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:34:25][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:34:26][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:34:59][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:34:59][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:34:59][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:34:59][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:34:59][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:34:59][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：15条
[INFO] [2022-04-26 14:35:09][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:35:09][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:35:09][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:35:09][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:35:09][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:35:12][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:35:12][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:35:12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:35:12][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:35:12][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:35:12][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:35:12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:35:15][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:35:35][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:35:35][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:35:35][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:35:35][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:35:35][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:35:36][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：15条
[INFO] [2022-04-26 14:35:48][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:35:48][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:35:48][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:35:48][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:35:49][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:35:52][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:35:52][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:35:52][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:35:52][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:35:52][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:35:52][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:35:52][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:35:57][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:39:08][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:39:08][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:39:08][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:39:08][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:39:08][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:39:08][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：15条
[INFO] [2022-04-26 14:40:26][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:40:26][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:40:27][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:40:27][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:40:27][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:40:27][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:40:27][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:40:27][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:40:27][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:40:27][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:40:27][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:40:27][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:40:31][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:40:54][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:40:54][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:40:54][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:40:54][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:42:27][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:43:04][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：15条
[INFO] [2022-04-26 14:50:47][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:50:47][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:50:47][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:50:47][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:50:47][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:50:47][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:50:47][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:50:47][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:50:47][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:50:47][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:50:47][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:50:47][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:50:47][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:50:52][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:50:52][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:50:52][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:50:52][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:51:48][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:51:48][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：2条
[INFO] [2022-04-26 14:52:26][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:52:26][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:52:27][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:52:27][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:52:27][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:52:27][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:52:27][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:52:27][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:52:27][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:52:27][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:52:27][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:52:27][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:52:27][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:52:29][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:52:30][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:52:30][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:52:30][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:52:59][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:52:59][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：1条
[INFO] [2022-04-26 14:53:45][kl.transmit.BatchTransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 14:53:45][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:53:46][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:53:46][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:53:46][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:53:46][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:53:46][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:53:46][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:53:46][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:53:46][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 14:53:46][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:53:46][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:53:46][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 14:53:48][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 14:53:48][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-26 14:53:48][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-26 14:53:48][kl.transmit.BatchTransmitJob]开始转发数据···
[INFO] [2022-04-26 14:54:03][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 14:54:05][kl.transmit.BatchTransmitJob]数据转发完成，已发送数据：1条
[INFO] [2022-04-26 16:49:26][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 16:49:28][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 16:49:28][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-26 16:49:29][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[WARN] [2022-04-26 16:51:18][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 16:51:20][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 16:51:22][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 16:51:24][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[INFO] [2022-04-26 16:57:18][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 16:57:18][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 16:57:20][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 16:57:20][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-26 16:57:20][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 16:57:20][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 16:57:20][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-26 16:57:20][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-26 16:57:20][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-26 16:57:20][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 16:57:20][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-26 16:57:20][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-26 16:57:20][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-26 16:57:23][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 41
[INFO] [2022-04-26 16:57:23][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-26 16:57:23][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 16:58:12][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 16:58:14][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 16:58:14][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-26 16:58:14][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[WARN] [2022-04-26 16:58:52][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 16:58:54][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 16:58:56][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 16:58:59][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 16:59:02][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 16:59:05][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 16:59:07][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 16:59:10][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 16:59:13][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 16:59:17][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 16:59:19][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 16:59:23][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 16:59:26][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 16:59:29][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 16:59:32][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 16:59:35][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 16:59:38][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 16:59:41][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 16:59:44][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[INFO] [2022-04-26 17:01:12][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 17:01:14][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 17:01:14][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-26 17:01:14][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 17:01:42][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 17:01:44][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 17:01:44][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-26 17:01:44][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 17:02:09][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 17:02:11][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 17:02:11][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-26 17:02:11][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 17:02:47][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 17:02:49][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 17:02:49][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-26 17:02:49][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 17:03:41][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 17:03:43][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 17:03:43][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-26 17:03:43][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 17:04:13][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 17:04:15][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 17:04:15][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-26 17:04:15][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 17:06:54][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 17:06:55][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 17:06:55][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-26 17:06:56][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 17:08:26][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 17:08:27][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 17:08:27][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-26 17:08:28][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[WARN] [2022-04-26 17:08:32][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:08:35][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:08:37][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:08:39][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:08:42][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:08:45][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:08:48][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:08:51][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:08:54][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:08:57][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:09:00][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:09:03][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:09:06][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:09:09][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:09:12][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:09:15][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:09:18][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:09:21][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:09:24][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:09:28][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:09:31][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:09:34][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:09:37][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:09:40][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:09:43][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[INFO] [2022-04-26 17:09:51][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 17:09:53][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 17:09:53][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[WARN] [2022-04-26 17:09:55][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:09:57][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:09:59][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[INFO] [2022-04-26 17:14:37][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 17:14:38][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 17:14:38][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[WARN] [2022-04-26 17:14:40][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:14:42][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[INFO] [2022-04-26 17:14:44][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[WARN] [2022-04-26 17:14:44][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Received invalid metadata error in produce request on partition test3-0 due to org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.. Going to request metadata update now
[WARN] [2022-04-26 17:14:44][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Received invalid metadata error in produce request on partition test3-0 due to org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.. Going to request metadata update now
[WARN] [2022-04-26 17:14:44][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Received invalid metadata error in produce request on partition test3-0 due to org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.. Going to request metadata update now
[WARN] [2022-04-26 17:14:44][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Received invalid metadata error in produce request on partition test3-0 due to org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.. Going to request metadata update now
[WARN] [2022-04-26 17:14:44][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Received invalid metadata error in produce request on partition test3-0 due to org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.. Going to request metadata update now
[WARN] [2022-04-26 17:14:44][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Received invalid metadata error in produce request on partition test3-0 due to org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.. Going to request metadata update now
[WARN] [2022-04-26 17:14:44][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Received invalid metadata error in produce request on partition test3-0 due to org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.. Going to request metadata update now
[WARN] [2022-04-26 17:14:44][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Received invalid metadata error in produce request on partition test3-0 due to org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.. Going to request metadata update now
[WARN] [2022-04-26 17:14:44][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Received invalid metadata error in produce request on partition test3-0 due to org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.. Going to request metadata update now
[WARN] [2022-04-26 17:14:44][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Received invalid metadata error in produce request on partition test3-0 due to org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.. Going to request metadata update now
[INFO] [2022-04-26 17:15:19][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 17:15:21][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 17:15:21][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-26 17:15:21][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 17:15:59][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 17:16:01][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 17:16:01][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-26 17:16:01][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-26 17:16:52][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 17:16:54][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 17:16:54][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-26 17:16:54][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[WARN] [2022-04-26 17:17:05][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Received invalid metadata error in produce request on partition test3-0 due to org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.. Going to request metadata update now
[WARN] [2022-04-26 17:17:07][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:17:09][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:17:11][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:17:13][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:17:16][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:17:19][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:17:22][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:17:25][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:17:28][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:17:32][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[INFO] [2022-04-26 17:18:38][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 17:18:40][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 17:18:40][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-26 17:18:40][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[WARN] [2022-04-26 17:18:57][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:18:59][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:19:02][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:19:04][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:19:07][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:19:10][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:19:13][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:19:16][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:19:19][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:19:22][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:19:25][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:19:28][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:19:31][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:19:34][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:19:37][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:19:40][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:19:43][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:19:46][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:19:49][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:19:52][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:19:55][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:19:58][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:20:01][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:20:04][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:20:07][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:20:10][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:20:13][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:20:16][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:20:19][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:20:22][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:20:26][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:20:29][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:20:32][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:20:35][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:20:38][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:20:41][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:20:44][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:20:47][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:20:50][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:20:53][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:20:56][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:20:59][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:21:02][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:21:05][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:21:08][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:21:11][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:21:14][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:21:17][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:21:20][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:21:23][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:21:26][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:21:29][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:21:32][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:21:35][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:21:38][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:21:41][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:21:44][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:21:47][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:21:50][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:21:53][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:21:56][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:21:59][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:22:02][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:22:05][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:22:08][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:22:12][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:22:15][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:22:18][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:22:21][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:22:24][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:22:27][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:22:30][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:22:33][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:22:36][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:22:39][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:22:42][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:22:45][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:22:48][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:22:51][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:22:54][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:22:57][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:23:00][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:23:03][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:23:06][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:23:09][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:23:12][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:23:15][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[INFO] [2022-04-26 17:23:53][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 17:23:54][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 17:23:54][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[WARN] [2022-04-26 17:23:56][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:23:58][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:24:00][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:24:03][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:24:05][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:24:08][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:24:11][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:24:14][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:24:17][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:24:20][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:24:23][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:24:26][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:24:29][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:24:32][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:24:35][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:24:38][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:24:41][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:24:44][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[INFO] [2022-04-26 17:26:58][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 17:26:59][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 17:26:59][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[WARN] [2022-04-26 17:27:01][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:27:03][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:27:05][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:27:08][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:27:10][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:27:13][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:27:16][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:27:19][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:27:22][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:27:25][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:27:28][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:27:31][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:27:34][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:27:38][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:27:41][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:27:44][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:27:47][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:27:50][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:27:54][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:27:56][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:27:59][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:28:02][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:28:05][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:28:09][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:28:12][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[INFO] [2022-04-26 17:28:59][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 17:29:01][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 17:29:01][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[WARN] [2022-04-26 17:29:03][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:29:05][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:29:07][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:29:09][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:29:12][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:29:15][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:29:18][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:29:21][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:29:24][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[INFO] [2022-04-26 17:32:21][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 17:32:23][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 17:32:23][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[WARN] [2022-04-26 17:32:25][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:32:27][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:32:29][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:32:32][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:32:34][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:32:37][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[INFO] [2022-04-26 17:32:57][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 17:32:59][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 17:32:59][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[WARN] [2022-04-26 17:33:01][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:33:03][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:33:05][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:33:08][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:33:10][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:33:13][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:33:16][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:33:19][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:33:22][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:33:25][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:33:28][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:33:31][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:33:34][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:33:37][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:33:40][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:33:43][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:33:47][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:33:50][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:33:53][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:33:56][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:33:59][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:34:02][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:34:05][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:34:08][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:34:11][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:34:14][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:34:17][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:34:20][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:34:24][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:34:27][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:34:30][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:34:33][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:34:36][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[INFO] [2022-04-26 17:34:50][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 17:34:52][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 17:34:52][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[WARN] [2022-04-26 17:34:54][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:34:56][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:34:58][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:35:01][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:35:03][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:35:06][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:35:09][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:35:12][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:35:15][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:35:18][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:35:21][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:35:24][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:35:28][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:35:30][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:35:34][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:35:37][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:35:40][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:35:43][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:35:46][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:35:49][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:35:52][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:35:55][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:35:58][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:36:01][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:36:04][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:36:08][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:36:11][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:36:14][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:36:17][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:36:20][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:36:23][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:36:26][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:36:29][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:36:32][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:36:35][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:36:38][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:36:41][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:36:44][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:36:47][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:36:50][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:36:53][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:36:56][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:36:59][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:37:02][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:37:05][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:37:08][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:37:11][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:37:14][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:37:17][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:37:20][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:37:23][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:37:27][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:37:30][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:37:33][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:37:36][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:37:39][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:37:42][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:37:45][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:37:48][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:37:51][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:37:54][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:37:57][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:38:00][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:38:03][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:38:07][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[INFO] [2022-04-26 17:40:41][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 17:40:43][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 17:40:43][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[WARN] [2022-04-26 17:40:45][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:40:47][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:40:49][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:40:51][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:40:54][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:40:57][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:41:00][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:41:03][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:41:06][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:41:09][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:41:12][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:41:15][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:41:19][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:41:22][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:41:25][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:41:28][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:41:31][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:41:34][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:41:37][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-26 17:41:40][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[INFO] [2022-04-26 18:00:13][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 18:00:13][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 18:00:15][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 18:00:15][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-26 18:00:15][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 18:00:15][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 18:00:15][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-26 18:00:15][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-26 18:00:15][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-26 18:00:16][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 18:00:16][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-26 18:00:16][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-26 18:00:16][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[ERROR] [2022-04-26 18:00:16][kl.transmit.TransmitJob]转发作业遇到异常，即将退出！
[INFO] [2022-04-26 18:00:19][org.apache.kafka.clients.producer.KafkaProducer][Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[INFO] [2022-04-26 18:00:36][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 18:00:36][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 18:00:38][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 18:00:38][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-26 18:00:38][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 18:00:38][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 18:00:38][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-26 18:00:38][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-26 18:00:38][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-26 18:00:38][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 18:00:38][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-26 18:00:38][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-26 18:00:38][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[ERROR] [2022-04-26 18:00:38][kl.transmit.TransmitJob]转发作业遇到异常，即将退出！
[INFO] [2022-04-26 18:00:49][org.apache.kafka.clients.producer.KafkaProducer][Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[INFO] [2022-04-26 18:01:07][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 18:01:07][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 18:01:08][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 18:01:08][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-26 18:01:08][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 18:01:08][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 18:01:08][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-26 18:01:08][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-26 18:01:08][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-26 18:01:12][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 18:01:12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-26 18:01:12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-26 18:01:12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[ERROR] [2022-04-26 18:01:29][kl.transmit.TransmitJob]转发作业遇到异常，即将退出！
[INFO] [2022-04-26 18:02:31][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 18:02:31][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 18:02:31][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 18:02:31][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-26 18:02:31][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 18:02:31][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 18:02:31][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-26 18:02:31][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-26 18:02:31][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-26 18:02:33][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 18:02:33][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-26 18:02:33][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-26 18:02:33][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-26 18:02:45][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 48
[INFO] [2022-04-26 18:02:45][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-26 18:03:42][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-26 18:03:42][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-26 18:03:43][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 18:03:43][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-26 18:03:43][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-26 18:03:43][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-26 18:03:43][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-26 18:03:43][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-26 18:03:43][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-26 18:03:45][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-26 18:03:45][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-26 18:03:45][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-26 18:03:45][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-26 18:03:49][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 49
[INFO] [2022-04-26 18:03:49][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-27 09:31:51][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-27 09:31:51][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 2
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-27 09:31:53][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 09:31:53][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 09:31:53][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 09:31:53][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 09:31:53][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 09:31:53][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-27 09:31:53][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-27 09:31:53][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 09:31:53][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-27 09:31:53][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-27 09:31:53][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-27 09:31:56][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 51
[INFO] [2022-04-27 09:31:56][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-27 09:32:02][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-27 09:32:57][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-27 09:32:58][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-27 09:32:59][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 09:32:59][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 09:32:59][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 09:32:59][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 09:32:59][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 09:32:59][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-27 09:32:59][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-27 09:33:00][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 09:33:00][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-27 09:33:00][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-27 09:33:00][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-27 09:33:08][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 52
[INFO] [2022-04-27 09:33:08][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-27 09:33:28][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-27 09:46:37][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 09:46:38][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 09:46:38][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 09:46:39][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 09:47:01][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-27 09:47:01][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-27 09:47:03][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 09:47:03][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 09:47:03][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 09:47:03][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 09:47:03][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 09:47:03][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-27 09:47:03][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-27 09:47:03][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 09:47:03][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-27 09:47:03][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-27 09:47:03][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-27 09:47:06][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 54
[INFO] [2022-04-27 09:47:06][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-27 09:47:06][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-27 09:54:28][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-27 09:54:28][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-27 09:54:30][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 09:54:30][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 09:54:30][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 09:54:30][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 09:54:30][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 09:54:30][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-27 09:54:30][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-27 09:54:30][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 09:54:30][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-27 09:54:30][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-27 09:54:30][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-27 09:54:33][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 56
[INFO] [2022-04-27 09:54:33][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-27 09:54:33][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-27 09:54:46][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 09:54:48][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 09:54:48][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 09:54:48][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:01:21][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-27 10:01:44][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-27 10:01:44][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-27 10:01:45][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:01:45][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:01:45][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:01:45][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:01:45][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:01:45][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-27 10:01:45][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-27 10:01:46][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:01:46][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-27 10:01:46][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-27 10:01:46][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-27 10:01:49][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 58
[INFO] [2022-04-27 10:01:49][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-27 10:01:50][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-27 10:01:51][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:01:53][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:01:53][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:01:53][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:02:18][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-27 10:02:18][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-27 10:02:20][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:02:20][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:02:20][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:02:20][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:02:20][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:02:20][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-27 10:02:20][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-27 10:02:20][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:02:20][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-27 10:02:20][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-27 10:02:20][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-27 10:02:34][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 59
[INFO] [2022-04-27 10:02:34][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-27 10:02:39][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-27 10:04:10][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-27 10:04:11][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-27 10:04:12][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:04:12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:04:12][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:04:12][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:04:12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:04:12][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-27 10:04:12][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-27 10:04:13][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:04:13][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-27 10:04:13][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-27 10:04:13][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-27 10:04:19][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 61
[INFO] [2022-04-27 10:04:19][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-27 10:04:24][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-27 10:05:45][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:05:47][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:05:47][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:05:47][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:07:24][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-27 10:07:24][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-27 10:07:26][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:07:26][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:07:26][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:07:26][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:07:26][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:07:26][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-27 10:07:26][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-27 10:07:26][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:07:26][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-27 10:07:26][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-27 10:07:26][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-27 10:07:29][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 63
[INFO] [2022-04-27 10:07:29][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-27 10:07:30][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-27 10:11:25][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:11:26][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:11:26][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:11:26][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:11:39][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-27 10:11:39][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-27 10:11:41][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:11:41][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:11:41][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:11:41][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:11:41][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:11:41][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-27 10:11:41][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-27 10:11:41][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:11:41][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-27 10:11:41][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-27 10:11:41][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-27 10:11:45][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 64
[INFO] [2022-04-27 10:11:45][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-27 10:11:46][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-27 10:16:36][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:16:37][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:16:37][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:16:38][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:17:11][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:17:13][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:17:13][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:17:13][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:18:38][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-27 10:18:38][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-27 10:18:39][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:18:39][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:18:39][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:18:39][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:18:39][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:18:39][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-27 10:18:39][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-27 10:18:40][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:18:40][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-27 10:18:40][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-27 10:18:40][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-27 10:18:43][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 66
[INFO] [2022-04-27 10:18:43][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-27 10:18:43][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-27 10:19:21][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:19:23][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:19:23][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:19:23][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:22:48][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:22:50][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:22:50][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:22:50][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:24:59][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-27 10:24:59][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-27 10:25:01][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:25:01][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:25:01][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:25:01][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:25:01][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:25:01][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-27 10:25:01][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-27 10:25:01][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:25:01][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-27 10:25:01][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-27 10:25:01][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-27 10:25:20][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-27 10:25:20][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-27 10:25:21][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:25:21][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:25:21][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:25:21][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:25:21][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:25:21][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-27 10:25:21][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-27 10:25:22][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:25:22][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-27 10:25:22][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-27 10:25:22][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-27 10:25:34][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 69
[INFO] [2022-04-27 10:25:34][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-27 10:25:34][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-27 10:27:34][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-27 10:27:34][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-27 10:27:36][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:27:36][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:27:36][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:27:36][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:27:36][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:27:36][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-27 10:27:36][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-27 10:27:36][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:27:36][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-27 10:27:36][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-27 10:27:36][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-27 10:27:39][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:27:39][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 71
[INFO] [2022-04-27 10:27:39][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-27 10:27:40][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:27:40][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:27:41][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:27:42][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-27 10:28:28][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-27 10:28:29][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-27 10:28:30][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:28:30][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:28:30][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:28:30][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:28:30][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:28:30][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-27 10:28:30][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-27 10:28:31][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:28:31][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-27 10:28:31][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-27 10:28:31][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-27 10:28:44][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 72
[INFO] [2022-04-27 10:28:44][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-27 10:28:45][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-27 10:29:17][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-27 10:29:17][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-27 10:29:19][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:29:19][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:29:19][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:29:19][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:29:19][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:29:19][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-27 10:29:19][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-27 10:29:19][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:29:19][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-27 10:29:19][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-27 10:29:19][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-27 10:29:29][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 74
[INFO] [2022-04-27 10:29:29][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-27 10:29:34][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-27 10:30:41][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-27 10:30:41][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-27 10:30:43][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:30:43][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:30:43][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:30:43][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:30:43][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:30:43][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-27 10:30:43][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-27 10:30:44][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:30:44][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-27 10:30:44][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-27 10:30:44][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-27 10:30:48][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 76
[INFO] [2022-04-27 10:30:48][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-27 10:30:49][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-27 10:31:11][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:31:12][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:31:12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:31:13][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:31:25][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:31:26][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:31:26][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:31:27][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:33:42][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:33:43][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:33:43][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:33:44][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:36:09][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-27 10:36:09][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-27 10:36:11][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:36:11][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:36:11][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:36:11][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:36:11][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:36:11][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-27 10:36:11][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-27 10:36:11][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:36:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-27 10:36:11][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-27 10:36:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-27 10:36:16][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 78
[INFO] [2022-04-27 10:36:16][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-27 10:36:17][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-27 10:40:18][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-27 10:40:18][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-27 10:40:20][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:40:20][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:40:20][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:40:20][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:40:20][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:40:20][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-27 10:40:20][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-27 10:40:20][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:40:20][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-27 10:40:20][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-27 10:40:20][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-27 10:40:23][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 80
[INFO] [2022-04-27 10:40:23][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-27 10:40:24][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-27 10:40:53][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-27 10:40:53][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-27 10:40:54][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:40:54][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:40:54][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:40:54][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:40:54][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:40:54][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-27 10:40:54][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-27 10:40:55][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:40:55][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-27 10:40:55][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-27 10:40:55][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-27 10:41:02][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 81
[INFO] [2022-04-27 10:41:02][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-27 10:54:14][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-27 10:54:14][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-27 10:54:16][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:54:16][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:54:16][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:54:16][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:54:16][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:54:16][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-27 10:54:16][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-27 10:54:16][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:54:16][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-27 10:54:16][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-27 10:54:16][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-27 10:54:19][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 83
[INFO] [2022-04-27 10:54:19][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-27 10:54:45][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:54:47][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:54:47][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:54:47][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:54:48][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-27 10:55:07][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:55:09][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:55:09][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:55:09][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:56:02][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-27 10:56:02][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-27 10:56:03][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:56:03][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:56:03][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:56:03][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:56:03][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:56:03][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-27 10:56:03][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-27 10:56:04][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:56:04][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-27 10:56:04][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-27 10:56:04][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-27 10:56:13][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 84
[INFO] [2022-04-27 10:56:13][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-27 10:56:13][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-27 10:56:35][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-27 10:56:35][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-27 10:56:37][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:56:37][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:56:37][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 10:56:37][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 10:56:37][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 10:56:37][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-27 10:56:37][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-27 10:56:37][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 10:56:37][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-27 10:56:37][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-27 10:56:37][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-27 10:56:49][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 85
[INFO] [2022-04-27 10:56:49][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-27 10:56:49][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-27 11:02:51][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-27 11:02:51][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-27 11:02:52][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 11:02:52][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 11:02:52][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-27 11:02:52][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0
[INFO] [2022-04-27 11:02:52][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : 3402a8361b734732
[INFO] [2022-04-27 11:02:52][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-27 11:02:52][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-27 11:02:53][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-27 11:02:53][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-27 11:02:53][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-27 11:02:53][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-27 11:02:56][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 87
[INFO] [2022-04-27 11:02:56][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-27 11:02:56][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
