[INFO] [2022-04-24 14:35:25][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit01
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 14:35:26][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 14:35:26][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 14:35:26][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 14:35:26][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 14:35:26][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 14:35:27][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 14:35:27][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Discovered group coordinator prod-bigdata-pc14:6667 (id: 2147482643 rack: null)
[INFO] [2022-04-24 14:35:27][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Revoking previously assigned partitions []
[INFO] [2022-04-24 14:35:27][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] (Re-)joining group
[INFO] [2022-04-24 14:35:30][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Successfully joined group with generation 1
[INFO] [2022-04-24 14:35:30][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 14:35:30][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-1, groupId=transmit01] Resetting offset for partition test5-0 to offset 13950.
[WARN] [2022-04-24 14:36:19][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:21][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:23][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:26][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:28][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:31][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:34][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:37][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:40][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:43][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:46][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:50][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:53][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:56][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:36:59][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:02][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:05][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:08][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:11][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:14][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:17][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:20][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:23][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:26][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:29][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:32][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:35][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:38][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:41][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:45][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:48][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[WARN] [2022-04-24 14:37:51][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node -1 could not be established. Broker may not be available.
[INFO] [2022-04-24 15:08:11][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit01
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 15:08:12][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:08:12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:08:12][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:08:13][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:08:13][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:08:13][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:08:13][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Discovered group coordinator prod-bigdata-pc14:6667 (id: 2147482643 rack: null)
[INFO] [2022-04-24 15:08:13][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:08:13][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] (Re-)joining group
[INFO] [2022-04-24 15:08:16][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Successfully joined group with generation 1
[INFO] [2022-04-24 15:08:16][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 15:08:16][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-1, groupId=transmit01] Resetting offset for partition test5-0 to offset 13951.
[INFO] [2022-04-24 15:08:19][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 15:11:48][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit01
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 15:11:50][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:11:50][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:11:50][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:11:50][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:11:50][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:11:50][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:11:50][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Discovered group coordinator prod-bigdata-pc14:6667 (id: 2147482643 rack: null)
[INFO] [2022-04-24 15:11:50][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:11:50][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] (Re-)joining group
[INFO] [2022-04-24 15:11:53][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Successfully joined group with generation 3
[INFO] [2022-04-24 15:11:53][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 15:11:53][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-1, groupId=transmit01] Resetting offset for partition test5-0 to offset 13953.
[INFO] [2022-04-24 15:12:24][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 15:13:02][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 15:13:04][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:13:04][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:13:04][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:13:04][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:13:04][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:13:04][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 15:13:04][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:13:04][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 15:13:04][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:13:07][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 1
[INFO] [2022-04-24 15:13:07][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 15:13:07][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Resetting offset for partition test5-0 to offset 13954.
[INFO] [2022-04-24 15:13:11][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 15:16:00][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 15:16:02][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:16:02][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:16:02][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:16:02][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:16:02][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:16:02][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:16:02][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 15:16:02][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:16:02][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 15:16:05][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 3
[INFO] [2022-04-24 15:16:05][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 15:16:05][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Resetting offset for partition test5-0 to offset 13962.
[INFO] [2022-04-24 15:19:11][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 15:19:13][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:19:13][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:19:13][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:19:13][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:19:13][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:19:13][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:19:13][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 15:19:13][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:19:13][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 15:19:36][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 4
[INFO] [2022-04-24 15:19:36][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 15:38:00][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 15:38:02][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:38:02][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:38:02][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:38:02][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:38:02][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:38:02][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:38:02][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 15:38:02][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:38:02][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 15:38:05][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 1
[INFO] [2022-04-24 15:38:05][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 15:39:56][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 15:39:58][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:39:58][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:39:58][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:39:58][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:39:58][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:39:58][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:39:58][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 15:39:58][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:39:58][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 15:40:01][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 3
[INFO] [2022-04-24 15:40:01][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 15:40:01][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Resetting offset for partition test5-0 to offset 13963.
[INFO] [2022-04-24 15:40:10][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[WARN] [2022-04-24 15:40:37][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 15:40:39][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 15:40:42][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 15:40:44][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 15:40:47][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 15:40:50][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 15:40:53][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 15:40:56][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 15:40:59][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 15:41:02][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 15:41:05][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[INFO] [2022-04-24 15:45:57][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 15:45:58][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:45:58][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:45:58][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:45:58][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:45:58][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:45:59][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:45:59][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 15:45:59][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:45:59][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 15:46:02][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 5
[INFO] [2022-04-24 15:46:02][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 15:46:02][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Resetting offset for partition test5-0 to offset 13967.
[INFO] [2022-04-24 15:50:10][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 15:50:12][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:50:12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:50:12][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:50:12][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:50:12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:50:12][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:50:12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 15:50:12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:50:12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 15:50:29][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 6
[INFO] [2022-04-24 15:50:29][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 15:50:29][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Resetting offset for partition test5-0 to offset 13970.
[INFO] [2022-04-24 15:51:12][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 15:54:09][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 15:54:11][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:54:11][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:54:11][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:54:11][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:54:11][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:54:11][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:54:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 15:54:11][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:54:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 15:54:14][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 8
[INFO] [2022-04-24 15:54:14][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 15:54:21][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 15:56:10][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[WARN] [2022-04-24 15:56:12][org.apache.kafka.clients.consumer.ConsumerConfig]The configuration 'ack-mode' was supplied but isn't a known config.
[INFO] [2022-04-24 15:56:12][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:56:12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:56:12][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:56:12][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:56:12][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:56:12][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:56:12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 15:56:12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:56:12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 15:56:32][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 9
[INFO] [2022-04-24 15:56:32][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 15:56:33][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 15:57:02][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 15:57:04][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:57:04][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:57:04][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:57:04][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:57:04][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:57:04][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:57:04][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 15:57:04][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:57:04][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 15:57:31][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[WARN] [2022-04-24 15:57:33][org.apache.kafka.clients.consumer.ConsumerConfig]The configuration 'ack-mode' was supplied but isn't a known config.
[INFO] [2022-04-24 15:57:33][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:57:33][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:57:33][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:57:33][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:57:33][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:57:33][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:57:33][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 15:57:33][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:57:33][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 15:57:51][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 11
[INFO] [2022-04-24 15:57:51][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 15:57:51][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 15:59:08][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 15:59:10][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:59:10][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:59:10][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:59:10][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:59:10][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:59:10][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:59:10][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 15:59:10][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:59:10][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 15:59:24][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 12
[INFO] [2022-04-24 15:59:24][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 15:59:24][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 15:59:46][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 15:59:48][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:59:48][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:59:48][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 15:59:48][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 15:59:48][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 15:59:48][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 15:59:48][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 15:59:48][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 15:59:48][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:00:06][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 13
[INFO] [2022-04-24 16:00:06][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:00:06][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 16:07:39][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 16:07:41][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:07:41][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:07:41][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 16:07:41][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:07:41][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:07:41][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 16:07:41][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 16:07:41][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 16:07:41][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:07:44][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 15
[INFO] [2022-04-24 16:07:44][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:07:44][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 16:08:09][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 16:08:11][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:08:11][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:08:11][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 16:08:11][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:08:11][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:08:11][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 16:08:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 16:08:11][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 16:08:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:08:32][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 16
[INFO] [2022-04-24 16:08:32][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:08:32][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 16:11:28][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 16:11:30][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:11:30][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:11:30][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 16:11:30][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:11:30][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:11:30][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 16:11:30][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 16:11:30][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 16:11:30][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:11:33][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 18
[INFO] [2022-04-24 16:11:33][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:11:33][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 16:17:34][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 16:17:36][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:17:36][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:17:36][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 16:17:36][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:17:36][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:17:37][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 16:17:37][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 16:17:37][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 16:17:37][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:17:40][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 20
[INFO] [2022-04-24 16:17:40][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:17:40][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[ERROR] [2022-04-24 16:17:41][org.apache.kafka.clients.producer.internals.ProducerBatch]Error executing user-provided callback on message for topic-partition 'test3-0'
java.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:2215)
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquireAndEnsureOpen(KafkaConsumer.java:2199)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1453)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1431)
	at kl.transmit.TransmitJob$1.onCompletion(TransmitJob.java:62)
	at org.apache.kafka.clients.producer.KafkaProducer$InterceptorCallback.onCompletion(KafkaProducer.java:1235)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.completeFutureAndFireCallbacks(ProducerBatch.java:201)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.done(ProducerBatch.java:187)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:599)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:575)
	at org.apache.kafka.clients.producer.internals.Sender.handleProduceResponse(Sender.java:485)
	at org.apache.kafka.clients.producer.internals.Sender.access$100(Sender.java:74)
	at org.apache.kafka.clients.producer.internals.Sender$1.onComplete(Sender.java:700)
	at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)
	at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:532)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:524)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:239)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:163)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] [2022-04-24 16:17:41][org.apache.kafka.clients.producer.internals.ProducerBatch]Error executing user-provided callback on message for topic-partition 'test3-0'
java.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:2215)
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquireAndEnsureOpen(KafkaConsumer.java:2199)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1453)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1431)
	at kl.transmit.TransmitJob$1.onCompletion(TransmitJob.java:62)
	at org.apache.kafka.clients.producer.KafkaProducer$InterceptorCallback.onCompletion(KafkaProducer.java:1235)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.completeFutureAndFireCallbacks(ProducerBatch.java:201)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.done(ProducerBatch.java:187)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:599)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:575)
	at org.apache.kafka.clients.producer.internals.Sender.handleProduceResponse(Sender.java:485)
	at org.apache.kafka.clients.producer.internals.Sender.access$100(Sender.java:74)
	at org.apache.kafka.clients.producer.internals.Sender$1.onComplete(Sender.java:700)
	at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)
	at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:532)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:524)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:239)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:163)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] [2022-04-24 16:17:41][org.apache.kafka.clients.producer.internals.ProducerBatch]Error executing user-provided callback on message for topic-partition 'test3-0'
java.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:2215)
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquireAndEnsureOpen(KafkaConsumer.java:2199)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1453)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1431)
	at kl.transmit.TransmitJob$1.onCompletion(TransmitJob.java:62)
	at org.apache.kafka.clients.producer.KafkaProducer$InterceptorCallback.onCompletion(KafkaProducer.java:1235)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.completeFutureAndFireCallbacks(ProducerBatch.java:201)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.done(ProducerBatch.java:187)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:599)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:575)
	at org.apache.kafka.clients.producer.internals.Sender.handleProduceResponse(Sender.java:485)
	at org.apache.kafka.clients.producer.internals.Sender.access$100(Sender.java:74)
	at org.apache.kafka.clients.producer.internals.Sender$1.onComplete(Sender.java:700)
	at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)
	at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:532)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:524)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:239)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:163)
	at java.lang.Thread.run(Thread.java:748)
[INFO] [2022-04-24 16:26:19][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 16:26:20][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:26:20][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:26:20][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 16:26:20][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:26:20][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:26:21][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 16:26:21][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 16:26:21][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 16:26:21][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:26:44][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 21
[INFO] [2022-04-24 16:26:44][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:26:44][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[ERROR] [2022-04-24 16:26:45][org.apache.kafka.clients.producer.internals.ProducerBatch]Error executing user-provided callback on message for topic-partition 'test3-0'
java.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:2215)
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquireAndEnsureOpen(KafkaConsumer.java:2199)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1453)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1431)
	at kl.transmit.TransmitJob$1.onCompletion(TransmitJob.java:65)
	at org.apache.kafka.clients.producer.KafkaProducer$InterceptorCallback.onCompletion(KafkaProducer.java:1235)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.completeFutureAndFireCallbacks(ProducerBatch.java:201)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.done(ProducerBatch.java:187)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:599)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:575)
	at org.apache.kafka.clients.producer.internals.Sender.handleProduceResponse(Sender.java:485)
	at org.apache.kafka.clients.producer.internals.Sender.access$100(Sender.java:74)
	at org.apache.kafka.clients.producer.internals.Sender$1.onComplete(Sender.java:700)
	at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)
	at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:532)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:524)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:239)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:163)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] [2022-04-24 16:26:45][org.apache.kafka.clients.producer.internals.ProducerBatch]Error executing user-provided callback on message for topic-partition 'test3-0'
java.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:2215)
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquireAndEnsureOpen(KafkaConsumer.java:2199)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1453)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1431)
	at kl.transmit.TransmitJob$1.onCompletion(TransmitJob.java:65)
	at org.apache.kafka.clients.producer.KafkaProducer$InterceptorCallback.onCompletion(KafkaProducer.java:1235)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.completeFutureAndFireCallbacks(ProducerBatch.java:201)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.done(ProducerBatch.java:187)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:599)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:575)
	at org.apache.kafka.clients.producer.internals.Sender.handleProduceResponse(Sender.java:485)
	at org.apache.kafka.clients.producer.internals.Sender.access$100(Sender.java:74)
	at org.apache.kafka.clients.producer.internals.Sender$1.onComplete(Sender.java:700)
	at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)
	at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:532)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:524)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:239)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:163)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] [2022-04-24 16:26:45][org.apache.kafka.clients.producer.internals.ProducerBatch]Error executing user-provided callback on message for topic-partition 'test3-0'
java.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:2215)
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquireAndEnsureOpen(KafkaConsumer.java:2199)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1453)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1431)
	at kl.transmit.TransmitJob$1.onCompletion(TransmitJob.java:65)
	at org.apache.kafka.clients.producer.KafkaProducer$InterceptorCallback.onCompletion(KafkaProducer.java:1235)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.completeFutureAndFireCallbacks(ProducerBatch.java:201)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.done(ProducerBatch.java:187)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:599)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:575)
	at org.apache.kafka.clients.producer.internals.Sender.handleProduceResponse(Sender.java:485)
	at org.apache.kafka.clients.producer.internals.Sender.access$100(Sender.java:74)
	at org.apache.kafka.clients.producer.internals.Sender$1.onComplete(Sender.java:700)
	at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)
	at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:532)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:524)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:239)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:163)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] [2022-04-24 16:26:45][org.apache.kafka.clients.producer.internals.ProducerBatch]Error executing user-provided callback on message for topic-partition 'test3-0'
java.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:2215)
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquireAndEnsureOpen(KafkaConsumer.java:2199)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1453)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitAsync(KafkaConsumer.java:1431)
	at kl.transmit.TransmitJob$1.onCompletion(TransmitJob.java:65)
	at org.apache.kafka.clients.producer.KafkaProducer$InterceptorCallback.onCompletion(KafkaProducer.java:1235)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.completeFutureAndFireCallbacks(ProducerBatch.java:201)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.done(ProducerBatch.java:187)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:599)
	at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:575)
	at org.apache.kafka.clients.producer.internals.Sender.handleProduceResponse(Sender.java:485)
	at org.apache.kafka.clients.producer.internals.Sender.access$100(Sender.java:74)
	at org.apache.kafka.clients.producer.internals.Sender$1.onComplete(Sender.java:700)
	at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)
	at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:532)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:524)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:239)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:163)
	at java.lang.Thread.run(Thread.java:748)
[INFO] [2022-04-24 16:29:25][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 16:29:27][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:29:27][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:29:27][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 16:29:27][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:29:27][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:29:27][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 16:29:27][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 16:29:27][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 16:29:27][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:29:30][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 23
[INFO] [2022-04-24 16:29:30][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:29:31][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 16:36:57][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 16:36:58][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:36:58][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:36:58][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 16:36:59][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:36:59][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:36:59][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 16:36:59][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 16:36:59][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 16:36:59][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:37:02][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 25
[INFO] [2022-04-24 16:37:02][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:37:18][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 16:38:09][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 16:38:11][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:38:11][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:38:11][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 16:38:11][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:38:11][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:38:11][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 16:38:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 16:38:11][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 16:38:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:38:14][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 27
[INFO] [2022-04-24 16:38:14][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:38:14][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 16:39:11][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 16:39:13][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:39:13][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:39:13][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 16:39:13][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:39:13][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:39:13][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 16:39:13][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 16:39:13][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 16:39:13][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:39:20][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 28
[INFO] [2022-04-24 16:39:20][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:39:20][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 16:41:02][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 16:41:03][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:41:03][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:41:03][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 16:41:03][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:41:03][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:41:04][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 16:41:04][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 16:41:04][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 16:41:04][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:41:23][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 29
[INFO] [2022-04-24 16:41:23][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:41:45][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 16:41:46][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:41:46][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:41:47][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 16:41:47][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:41:47][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:41:47][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 16:41:47][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 16:41:47][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 16:41:47][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:42:05][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 30
[INFO] [2022-04-24 16:42:05][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:42:06][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 16:48:02][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 16:48:04][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:48:04][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:48:04][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 16:48:04][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:48:04][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:48:04][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 16:48:04][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 16:48:04][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 16:48:04][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:48:12][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 31
[INFO] [2022-04-24 16:48:12][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:48:12][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 16:49:15][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 16:49:17][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:49:17][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:49:17][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 16:49:17][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 16:49:17][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 16:49:17][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 16:49:17][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 16:49:17][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 16:49:17][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 16:49:31][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 32
[INFO] [2022-04-24 16:49:31][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 16:49:31][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 17:13:51][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 17:13:52][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 17:13:52][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 17:13:52][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 17:13:53][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 17:13:53][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 17:13:53][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 17:13:53][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 17:13:53][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 17:13:53][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 17:14:16][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 33
[INFO] [2022-04-24 17:14:16][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 17:14:16][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[WARN] [2022-04-24 17:14:55][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:14:57][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:00][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:02][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:05][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:08][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:11][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:15][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:18][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:21][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:24][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:27][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:30][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:33][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:36][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:39][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:42][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:45][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:15:47][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Error while fetching metadata with correlation id 11 : {test3=LEADER_NOT_AVAILABLE}
[WARN] [2022-04-24 17:15:47][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Got error produce response with correlation id 12 on topic-partition test3-0, retrying (9 attempts left). Error: NOT_LEADER_FOR_PARTITION
[WARN] [2022-04-24 17:15:47][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Received invalid metadata error in produce request on partition test3-0 due to org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.. Going to request metadata update now
[WARN] [2022-04-24 17:15:47][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Got error produce response with correlation id 14 on topic-partition test3-0, retrying (8 attempts left). Error: NOT_LEADER_FOR_PARTITION
[WARN] [2022-04-24 17:15:47][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Received invalid metadata error in produce request on partition test3-0 due to org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.. Going to request metadata update now
[WARN] [2022-04-24 17:16:06][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:09][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:11][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:13][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:16][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:19][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:22][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:26][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:29][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:31][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:34][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:37][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:41][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:44][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:47][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:50][org.apache.kafka.clients.NetworkClient][Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
[WARN] [2022-04-24 17:16:52][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Got error produce response with correlation id 21 on topic-partition test3-0, retrying (9 attempts left). Error: NOT_LEADER_FOR_PARTITION
[WARN] [2022-04-24 17:16:52][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Received invalid metadata error in produce request on partition test3-0 due to org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.. Going to request metadata update now
[WARN] [2022-04-24 17:16:52][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Got error produce response with correlation id 23 on topic-partition test3-0, retrying (8 attempts left). Error: NOT_LEADER_FOR_PARTITION
[WARN] [2022-04-24 17:16:52][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Received invalid metadata error in produce request on partition test3-0 due to org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.. Going to request metadata update now
[WARN] [2022-04-24 17:16:52][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Got error produce response with correlation id 25 on topic-partition test3-0, retrying (7 attempts left). Error: NOT_LEADER_FOR_PARTITION
[WARN] [2022-04-24 17:16:52][org.apache.kafka.clients.producer.internals.Sender][Producer clientId=producer-1] Received invalid metadata error in produce request on partition test3-0 due to org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.. Going to request metadata update now
[INFO] [2022-04-24 17:20:27][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 17:20:28][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 17:20:28][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 17:20:28][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 17:20:28][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 17:20:28][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 17:20:29][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 17:20:29][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 17:20:29][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 17:20:29][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 17:20:32][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 35
[INFO] [2022-04-24 17:20:32][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 17:20:32][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-24 17:52:40][kl.transmit.TransmitJob]当前使用的kafka消费者配置：consumer-hdp-dev,生产者配置：producer-local
[INFO] [2022-04-24 17:52:40][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [prod-bigdata-pc2:6667, prod-bigdata-pc3:6667, prod-bigdata-pc4:6667, prod-bigdata-pc14:6667, prod-bigdata-pc15:6667]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit-hdp-dev
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-24 17:52:42][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 17:52:42][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 17:52:42][org.apache.kafka.clients.producer.ProducerConfig]ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1000
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[INFO] [2022-04-24 17:52:42][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-24 17:52:42][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-24 17:52:42][kl.transmit.TransmitJob]输入主题：test5,输出主题：test3
[INFO] [2022-04-24 17:52:42][kl.transmit.TransmitJob]输入主题订阅成功
[INFO] [2022-04-24 17:52:42][org.apache.kafka.clients.Metadata]Cluster ID: vONEJb7wSxS9dnIqH8pX2g
[INFO] [2022-04-24 17:52:42][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Discovered group coordinator prod-bigdata-pc4:6667 (id: 2147482646 rack: null)
[INFO] [2022-04-24 17:52:42][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Revoking previously assigned partitions []
[INFO] [2022-04-24 17:52:42][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] (Re-)joining group
[INFO] [2022-04-24 17:52:46][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Successfully joined group with generation 37
[INFO] [2022-04-24 17:52:46][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit-hdp-dev] Setting newly assigned partitions [test5-0]
[INFO] [2022-04-24 17:53:14][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-25 15:51:45][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit01
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-25 15:51:45][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit01
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-25 15:51:45][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit01
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-25 15:51:46][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-25 15:51:46][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-25 15:51:46][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-25 15:51:46][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-25 15:51:46][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-25 15:51:46][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-25 15:51:47][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-25 15:51:47][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-25 15:51:47][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-2, groupId=transmit01] Discovered group coordinator Mi-Lixz:9092 (id: 2147483647 rack: null)
[INFO] [2022-04-25 15:51:47][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Discovered group coordinator Mi-Lixz:9092 (id: 2147483647 rack: null)
[INFO] [2022-04-25 15:51:47][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Revoking previously assigned partitions []
[INFO] [2022-04-25 15:51:47][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-2, groupId=transmit01] Revoking previously assigned partitions []
[INFO] [2022-04-25 15:51:47][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] (Re-)joining group
[INFO] [2022-04-25 15:51:47][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-2, groupId=transmit01] (Re-)joining group
[INFO] [2022-04-25 15:51:47][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-2, groupId=transmit01] Successfully joined group with generation 1
[INFO] [2022-04-25 15:51:47][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-2, groupId=transmit01] Setting newly assigned partitions []
[INFO] [2022-04-25 15:51:47][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Successfully joined group with generation 1
[INFO] [2022-04-25 15:51:47][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Setting newly assigned partitions [test3-0]
[INFO] [2022-04-25 15:51:47][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-1, groupId=transmit01] Resetting offset for partition test3-0 to offset 105.
[INFO] [2022-04-25 15:52:18][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit01
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-25 15:52:18][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit01
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-25 15:52:18][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit01
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-25 15:52:20][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-25 15:52:20][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-25 15:52:20][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-25 15:52:20][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-25 15:52:20][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-25 15:52:20][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-25 15:52:20][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-25 15:52:20][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-25 15:52:20][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-25 15:52:20][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-3, groupId=transmit01] Discovered group coordinator Mi-Lixz:9092 (id: 2147483647 rack: null)
[INFO] [2022-04-25 15:52:20][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-2, groupId=transmit01] Discovered group coordinator Mi-Lixz:9092 (id: 2147483647 rack: null)
[INFO] [2022-04-25 15:52:20][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Discovered group coordinator Mi-Lixz:9092 (id: 2147483647 rack: null)
[INFO] [2022-04-25 15:52:20][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-3, groupId=transmit01] Revoking previously assigned partitions []
[INFO] [2022-04-25 15:52:20][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-2, groupId=transmit01] Revoking previously assigned partitions []
[INFO] [2022-04-25 15:52:20][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Revoking previously assigned partitions []
[INFO] [2022-04-25 15:52:20][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-2, groupId=transmit01] (Re-)joining group
[INFO] [2022-04-25 15:52:20][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-3, groupId=transmit01] (Re-)joining group
[INFO] [2022-04-25 15:52:20][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] (Re-)joining group
[INFO] [2022-04-25 15:52:29][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-2, groupId=transmit01] Successfully joined group with generation 2
[INFO] [2022-04-25 15:52:29][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-2, groupId=transmit01] Setting newly assigned partitions []
[INFO] [2022-04-25 15:52:29][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-3, groupId=transmit01] Successfully joined group with generation 2
[INFO] [2022-04-25 15:52:29][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-3, groupId=transmit01] Setting newly assigned partitions []
[INFO] [2022-04-25 15:52:29][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Successfully joined group with generation 2
[INFO] [2022-04-25 15:52:29][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit01] Setting newly assigned partitions [test3-0]
[INFO] [2022-04-25 15:52:29][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-1, groupId=transmit01] Resetting offset for partition test3-0 to offset 105.
[INFO] [2022-04-25 15:59:09][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit0Thread-0
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-25 15:59:09][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit0Thread-1
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-25 15:59:09][org.apache.kafka.clients.consumer.ConsumerConfig]ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = transmit0Thread-2
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[INFO] [2022-04-25 15:59:10][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-25 15:59:10][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-25 15:59:10][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-25 15:59:10][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-25 15:59:10][org.apache.kafka.common.utils.AppInfoParser]Kafka version : 2.0.0.3.1.5.0-152
[INFO] [2022-04-25 15:59:10][org.apache.kafka.common.utils.AppInfoParser]Kafka commitId : a10a6e16779f1930
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.Metadata]Cluster ID: TGkUKFGtRy-ODEMH7whcJA
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-2, groupId=transmit0Thread-1] Discovered group coordinator Mi-Lixz:9092 (id: 2147483647 rack: null)
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit0Thread-0] Discovered group coordinator Mi-Lixz:9092 (id: 2147483647 rack: null)
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-3, groupId=transmit0Thread-2] Discovered group coordinator Mi-Lixz:9092 (id: 2147483647 rack: null)
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-3, groupId=transmit0Thread-2] Revoking previously assigned partitions []
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit0Thread-0] Revoking previously assigned partitions []
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-2, groupId=transmit0Thread-1] Revoking previously assigned partitions []
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit0Thread-0] (Re-)joining group
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-3, groupId=transmit0Thread-2] (Re-)joining group
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-2, groupId=transmit0Thread-1] (Re-)joining group
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-1, groupId=transmit0Thread-0] Successfully joined group with generation 1
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-1, groupId=transmit0Thread-0] Setting newly assigned partitions [test3-0]
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-2, groupId=transmit0Thread-1] Successfully joined group with generation 1
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-2, groupId=transmit0Thread-1] Setting newly assigned partitions [test3-0]
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-1, groupId=transmit0Thread-0] Resetting offset for partition test3-0 to offset 106.
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-2, groupId=transmit0Thread-1] Resetting offset for partition test3-0 to offset 106.
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.AbstractCoordinator][Consumer clientId=consumer-3, groupId=transmit0Thread-2] Successfully joined group with generation 1
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator][Consumer clientId=consumer-3, groupId=transmit0Thread-2] Setting newly assigned partitions [test3-0]
[INFO] [2022-04-25 15:59:11][org.apache.kafka.clients.consumer.internals.Fetcher][Consumer clientId=consumer-3, groupId=transmit0Thread-2] Resetting offset for partition test3-0 to offset 106.
